# Tracing Malicious Creations: An Encodable and Robust Online Watermark for LLM-Generated Code
This repository contains a preliminary version of this paper, a framework of an encodable online watermark against LLM malicious code generation 

## Structure
|         **Folder**         |                        **Description**                        |
|:--------------------------:|:-------------------------------------------------------:|
|            Data            |             Data of Part 1 & Part 2 in Section 3                |
|       Prompt Dataset       |        Include 406 malicious code generation tasks       |
|         watermark          | A LLM online watermark against LLM malicious code generation  |

## Data
<img width="630"  alt="image" src="Others/data.png" align=center >

Part 1: Malicious code generated by LLM. Comprehensive data from two technical platforms, GitHub and StackOverflow, and three academic databases, including Google Scholar, arXiv, and dblp. Also include results from Google. 

Part 2: Malicious code not generated by LLM. Collecting repositories related to malicious code on GitHub.
## Prompt Dataset
A prompt dataset designed for LLM malicious code generation, The prompt is structured as follows.
<img width="630" align=center alt="prompt" src="Others/prompt.png">

Note that this dataset is for research and educational purposes only. Do not use it for malicious behavior.
## Watermark
You can refer to `requirements` to install all packages.
```
pip install requirements -r
```
and quick start:
```
python watermark.py
```
Note that you need to prepare Code LLM in advance and configure the relevant runtime environment and parameters. You may be able to find the most suitable configuration for you from the following materials: https://huggingface.co/docs/transformers/main_classes/tokenizer and https://huggingface.co/learn/nlp-course/chapter1/1
